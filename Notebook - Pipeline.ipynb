{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e09f87",
   "metadata": {},
   "source": [
    "# Klarna Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996787cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c6439",
   "metadata": {},
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_info():\n",
    "    colInfo = 'CaseStudyCols.csv'\n",
    "\n",
    "    with open(colInfo, newline='') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=';')\n",
    "        # skip over the first row\n",
    "        reader.__next__()\n",
    "        rows = list(reader)\n",
    "\n",
    "    col_names_types = {row[0]: row[1] for row in rows}\n",
    "\n",
    "    return col_names_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note that in reality the column 'default' and\n",
    "'worst_status_active_inv' as actually ints, but\n",
    "since they have NA values, we load them as objects\n",
    "for now.\n",
    "\n",
    "But note that the entries where default = NA are actually our\n",
    "entries that we are supposed to predict (validate set?)\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('dataset.csv', delimiter=';', dtype = get_col_info(), keep_default_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd302016",
   "metadata": {},
   "source": [
    "## Data Processing & Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2480a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out the NA entries for the column 'default'\n",
    "# First we save the entries to a seperate dataframe\n",
    "\n",
    "defaults_df = df[df['default'].isna().copy()]\n",
    "\n",
    "df = df[df['default'].notna().copy()]\n",
    "df['default'] = pd.to_numeric(df['default']).astype('int32')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8678c3c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455d711",
   "metadata": {},
   "source": [
    "## Columns with higher percentage of NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdef45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset, in particular look at the NA counts\n",
    "has_na = []\n",
    "for col in df.columns:\n",
    "    perc_na = 0\n",
    "    perc_na = round(df[col].isna().sum()/len(df), 2)\n",
    "    if perc_na > 0:\n",
    "        has_na.append([col, perc_na])\n",
    "\n",
    "\n",
    "# Plot out the columns with NA (their percentages) - no need to show it each time\n",
    "# fig, ax = plt.subplots(figsize=(10, 5), dpi=100)\n",
    "# na_plot = sns.barplot(x=[n[0] for n in has_na], y=[n[1] for n in has_na],  ax=ax)\n",
    "# na_plot.set_xticklabels(na_plot.get_xticklabels(), rotation=90, horizontalalignment='right')\n",
    "# na_plot.set_title(\"% of values = NA\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's explore some of the columns (other than default)\n",
    "## Look in particular at the columns with high NA.\n",
    "## Do we need those columns or can we simply drop them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a list of columns that have high % NA\n",
    "# % NA threshhold for columns to ignore\n",
    "na_threshold = .5\n",
    "cols_to_exclude = [col[0] for col in has_na if col[1] > na_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can see that 'merchant_category', 'merchant_group'\n",
    "## and 'name_in_email' are categorical (object = string)\n",
    "## These will either need to be ignored (not a good idea)\n",
    "## or encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd663752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the two 'merchant_' columns\n",
    "for c in df.columns:\n",
    "    if(c[:8] == 'merchant'):\n",
    "            print(df[c].value_counts())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1f5f3",
   "metadata": {},
   "source": [
    "## Look for correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c04c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(df):\n",
    "    correlation_matrix = df.corr()\n",
    "    column_names = correlation_matrix.columns\n",
    "\n",
    "    # Convert the correlation matrix into a DataFrame\n",
    "    corr_df = correlation_matrix.stack().reset_index()\n",
    "\n",
    "    # Rename the columns\n",
    "    corr_df.columns = ['feature_1','feature_2', 'correlation']\n",
    "\n",
    "    # Remove \"self correlations\"\n",
    "    no_self_correlation = (corr_df['feature_1'] != corr_df['feature_2'])\n",
    "    corr_df = corr_df[no_self_correlation]\n",
    "\n",
    "    # Absolute correlation\n",
    "    corr_df['absolute_correlation'] = np.abs(corr_df['correlation'])\n",
    "\n",
    "    # Correlation by pairs of features\n",
    "    return corr_df.sort_values(by=\"absolute_correlation\", ascending=False)\n",
    "\n",
    "corr_matrix(df).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are already 36 columns in the original dataset. A bit less after we\n",
    "## removed the high percentage NA columns. Before getting into the pipeline\n",
    "## let's remove some columns that are probably not needed.\n",
    "\n",
    "## First, columns that are highly correlated (corr > .8)\n",
    "cols_to_exclude +=  ['max_paid_inv_0_24m', 'num_arch_ok_0_12m', 'status_max_archived_0_24_months']\n",
    "\n",
    "## Next, categorical columns that 'probably' don't add much value\n",
    "## We still have the 'merchant_group' column that should be usefule\n",
    "cols_to_exclude += ['merchant_category', 'name_in_email']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa247e68",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude += ['default', 'uuid']\n",
    "\n",
    "# Defining the features and the target\n",
    "X = df.drop(columns=cols_to_exclude)\n",
    "y = df['default']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute then scale numerical values:\n",
    "num_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "    ('standard_scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Encode categorical values\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Parallelize \"num_transformer\" and \"cat_transfomer\"\n",
    "numeric_cols = X.select_dtypes(include='number').describe().columns.to_list()\n",
    "cat_cols = X.select_dtypes(include=['object', 'bool']).describe().columns.to_list()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_transformer', num_transformer, numeric_cols),\n",
    "    ('cat_transformer', cat_transformer, cat_cols),\n",
    "] )\n",
    "\n",
    "# Add estimator\n",
    "pipeline = make_pipeline(preprocessor, Ridge())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "print(\"Original training set\")\n",
    "display(X_train.head(5))\n",
    "\n",
    "print(\"Preprocessed training set\")\n",
    "transformed_df = pd.DataFrame(\n",
    "    X_train_transformed,\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ")\n",
    "\n",
    "transformed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reality is that the scaling/encoding should not change the correlations\n",
    "# corr_matrix(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac85305",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Pipeline\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions\n",
    "pipeline.predict(X_test.iloc[0:1])\n",
    "\n",
    "# Score model\n",
    "pipeline.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
